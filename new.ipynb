{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Lung Nodule Segmentation with EfficientUNet\n",
    "\n",
    "This script trains a deep learning model to segment lung nodules in CT scans\n",
    "using the LIDC-IDRI dataset.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import init\n",
    "\n",
    "import kagglehub\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm.notebook import tqdm  # Specifically optimized for Jupyter notebooks\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Dataset Classes\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class LIDCDataset:\n",
    "    def __init__(self, base_path, task='detection', transform=None, split='train', \n",
    "                 val_ratio=0.15, test_ratio=0.15, seed=42):\n",
    "        \"\"\"\n",
    "        LIDC-IDRI dataset loader\n",
    "        \n",
    "        Args:\n",
    "            base_path: Path to the dataset root\n",
    "            task: 'detection' or 'classification'\n",
    "            transform: Optional transforms to apply\n",
    "            split: 'train', 'val', or 'test'\n",
    "            val_ratio: Percentage of data for validation\n",
    "            test_ratio: Percentage of data for testing\n",
    "            seed: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.base_path = base_path\n",
    "        self.task = task\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        \n",
    "        # Collect all patients and nodules\n",
    "        self.samples = []\n",
    "        self._collect_dataset()\n",
    "        \n",
    "        # Split into train/val/test\n",
    "        self._create_splits(val_ratio, test_ratio, seed)\n",
    "    \n",
    "    def _collect_dataset(self):\n",
    "        \"\"\"Collect all image and mask pairs from the dataset\"\"\"\n",
    "        slices_dir = os.path.join(self.base_path, \"LIDC-IDRI-slices\")\n",
    "        \n",
    "        # Loop through all patients\n",
    "        for patient_id in os.listdir(slices_dir):\n",
    "            patient_path = os.path.join(slices_dir, patient_id)\n",
    "            if not os.path.isdir(patient_path):\n",
    "                continue\n",
    "                \n",
    "            # Loop through nodules for this patient\n",
    "            for nodule_id in os.listdir(patient_path):\n",
    "                nodule_path = os.path.join(patient_path, nodule_id)\n",
    "                if not os.path.isdir(nodule_path):\n",
    "                    continue\n",
    "                    \n",
    "                # Get images and masks for this nodule\n",
    "                images_path = os.path.join(nodule_path, \"images\")\n",
    "                if not os.path.exists(images_path):\n",
    "                    continue\n",
    "                    \n",
    "                # Check for all 4 annotator masks\n",
    "                mask_paths = []\n",
    "                for i in range(4):\n",
    "                    mask_path = os.path.join(nodule_path, f\"mask-{i}\")\n",
    "                    if os.path.exists(mask_path):\n",
    "                        mask_paths.append(mask_path)\n",
    "                \n",
    "                # Skip if no masks available\n",
    "                if not mask_paths:\n",
    "                    continue\n",
    "                \n",
    "                # Collect all slices\n",
    "                for img_file in os.listdir(images_path):\n",
    "                    if not img_file.endswith('.png'):\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract slice number from filename (e.g., \"slice-0.png\" -> 0)\n",
    "                    slice_num = int(img_file.split('-')[1].split('.')[0])\n",
    "                    \n",
    "                    # Find corresponding masks\n",
    "                    masks = []\n",
    "                    for mask_path in mask_paths:\n",
    "                        mask_file = os.path.join(mask_path, img_file)\n",
    "                        if os.path.exists(mask_file):\n",
    "                            masks.append(mask_file)\n",
    "                    \n",
    "                    # Skip if no matching masks\n",
    "                    if not masks:\n",
    "                        continue\n",
    "                    \n",
    "                    # Add sample\n",
    "                    self.samples.append({\n",
    "                        'patient_id': patient_id,\n",
    "                        'nodule_id': nodule_id,\n",
    "                        'slice_num': slice_num,\n",
    "                        'image_path': os.path.join(images_path, img_file),\n",
    "                        'mask_paths': masks\n",
    "                    })\n",
    "        \n",
    "        print(f\"Collected {len(self.samples)} valid image-mask pairs\")\n",
    "    \n",
    "    def _create_splits(self, val_ratio, test_ratio, seed):\n",
    "        \"\"\"Create train/val/test splits based on patient IDs to avoid data leakage\"\"\"\n",
    "        # Get unique patient IDs\n",
    "        patient_ids = list(set(s['patient_id'] for s in self.samples))\n",
    "        \n",
    "        # Split patient IDs\n",
    "        train_ids, test_ids = train_test_split(\n",
    "            patient_ids, test_size=test_ratio, random_state=seed\n",
    "        )\n",
    "        \n",
    "        train_ids, val_ids = train_test_split(\n",
    "            train_ids, test_size=val_ratio/(1-test_ratio), random_state=seed\n",
    "        )\n",
    "        \n",
    "        # Filter samples based on split\n",
    "        if self.split == 'train':\n",
    "            self.samples = [s for s in self.samples if s['patient_id'] in train_ids]\n",
    "        elif self.split == 'val':\n",
    "            self.samples = [s for s in self.samples if s['patient_id'] in val_ids]\n",
    "        elif self.split == 'test':\n",
    "            self.samples = [s for s in self.samples if s['patient_id'] in test_ids]\n",
    "        \n",
    "        print(f\"Split: {self.split}, Samples: {len(self.samples)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Load and return a sample\"\"\"\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = np.array(Image.open(sample['image_path']))\n",
    "        \n",
    "        # Load masks (using first annotator by default)\n",
    "        mask = np.array(Image.open(sample['mask_paths'][0]))\n",
    "        \n",
    "        # Ensure mask is normalized to [0,1] range\n",
    "        if mask.dtype == np.uint8:\n",
    "            mask = mask / 255.0\n",
    "        \n",
    "        # Load all masks if needed\n",
    "        if self.task == 'classification':\n",
    "            # For classification, we might want all annotator opinions\n",
    "            all_masks = [np.array(Image.open(mask_path)) / 255.0 for mask_path in sample['mask_paths']]\n",
    "            consensus_mask = np.zeros_like(all_masks[0])\n",
    "            for m in all_masks:\n",
    "                consensus_mask += m\n",
    "            # Consider a pixel part of nodule if at least 2 annotators agree\n",
    "            consensus_mask = (consensus_mask >= 2).astype(np.float32)\n",
    "            mask = consensus_mask\n",
    "        \n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed['image']\n",
    "            mask = transformed['mask']\n",
    "            \n",
    "        # Add channel dimension if needed and convert to tensor if not already\n",
    "        if not torch.is_tensor(image):\n",
    "            if len(image.shape) == 2:\n",
    "                image = image[np.newaxis, ...]\n",
    "            image = torch.from_numpy(image.astype(np.float32))\n",
    "        \n",
    "        if not torch.is_tensor(mask):\n",
    "            if len(mask.shape) == 2:\n",
    "                mask = mask[np.newaxis, ...]\n",
    "            mask = torch.from_numpy(mask.astype(np.float32))\n",
    "        \n",
    "        # For classification task, we might want to return additional info\n",
    "        metadata = {\n",
    "            'patient_id': sample['patient_id'],\n",
    "            'nodule_id': sample['nodule_id'],\n",
    "            'slice_num': sample['slice_num']\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'image': image, \n",
    "            'mask': mask,\n",
    "            'metadata': metadata\n",
    "        }\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Return dataset statistics\"\"\"\n",
    "        num_patients = len(set(s['patient_id'] for s in self.samples))\n",
    "        num_nodules = len(set((s['patient_id'], s['nodule_id']) for s in self.samples))\n",
    "        \n",
    "        return {\n",
    "            'num_samples': len(self.samples),\n",
    "            'num_patients': num_patients,\n",
    "            'num_nodules': num_nodules\n",
    "        }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Augmentation Functions\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def get_training_transforms(p=0.5):\n",
    "    return A.Compose([\n",
    "        # Spatial transforms (maintain anatomical context)\n",
    "        A.RandomRotate90(p=p),\n",
    "        A.HorizontalFlip(p=p),\n",
    "        A.VerticalFlip(p=p),\n",
    "        A.ShiftScaleRotate(\n",
    "            shift_limit=0.05,\n",
    "            scale_limit=0.05,\n",
    "            rotate_limit=15,\n",
    "            p=p\n",
    "        ),\n",
    "        \n",
    "        # Mild elastic deformation - reduced probability\n",
    "        A.ElasticTransform(\n",
    "            alpha=1.0,\n",
    "            sigma=50,\n",
    "            p=0.2,  # Reduced from 0.3\n",
    "        ),\n",
    "        \n",
    "        # Noise augmentation\n",
    "        A.GaussNoise(\n",
    "            std_range=(0.01, 0.05),  # Adjusted for less noise\n",
    "            mean_range=(0.0, 0.01),\n",
    "            p=0.1\n",
    "        ),\n",
    "        \n",
    "        # Very mild contrast/brightness\n",
    "        A.RandomBrightnessContrast(\n",
    "            brightness_limit=0.02,\n",
    "            contrast_limit=0.02,\n",
    "            p=0.1\n",
    "        ),\n",
    "        \n",
    "        # Standard preprocessing\n",
    "        A.Normalize(mean=0.5, std=0.5),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "def get_validation_transforms():\n",
    "    \"\"\"Only normalization and conversion to tensor, no augmentation\"\"\"\n",
    "    return A.Compose([\n",
    "        A.Normalize(mean=0.5, std=0.5),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "class LIDCDatasetWithAugmentation(LIDCDataset):\n",
    "    def __init__(self, base_path, task='detection', split='train', \n",
    "                 val_ratio=0.15, test_ratio=0.15, seed=42):\n",
    "        \"\"\"\n",
    "        LIDC-IDRI dataset with automatic augmentation\n",
    "        \n",
    "        This class extends LIDCDataset to automatically apply the appropriate\n",
    "        transforms based on the split (training or validation/test)\n",
    "        \"\"\"\n",
    "        # Set appropriate transforms based on split\n",
    "        if split == 'train':\n",
    "            transform = get_training_transforms(p=0.5)\n",
    "        else:\n",
    "            transform = get_validation_transforms()\n",
    "        \n",
    "        # Initialize parent class with the transform\n",
    "        super().__init__(base_path, task, transform, split, val_ratio, test_ratio, seed)\n",
    "\n",
    "def visualize_augmentations(dataset, idx=0, num_samples=5):\n",
    "    sample = dataset.samples[idx]\n",
    "    image = np.array(Image.open(sample['image_path']))\n",
    "    mask = np.array(Image.open(sample['mask_paths'][0]))\n",
    "    \n",
    "    # Create a figure with rows showing different augmentations\n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(10, num_samples*3))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Apply a random augmentation\n",
    "        if i == 0:\n",
    "            # First row shows original\n",
    "            aug_image, aug_mask = image, mask\n",
    "            axes[i, 0].set_title(\"Original Image\")\n",
    "            axes[i, 1].set_title(\"Original Mask\")\n",
    "        else:\n",
    "            # Apply augmentation\n",
    "            transform = get_training_transforms(p=1.0)  # Force augmentation\n",
    "            augmented = transform(image=image, mask=mask)\n",
    "            \n",
    "            # Convert from tensor to numpy and handle dimensions\n",
    "            if torch.is_tensor(augmented['image']):\n",
    "                aug_image = augmented['image'].numpy()\n",
    "                # Handle channel dimension correctly (C,H,W) -> (H,W,C) or just squeeze\n",
    "                if len(aug_image.shape) == 3:\n",
    "                    aug_image = np.transpose(aug_image, (1, 2, 0))\n",
    "            else:\n",
    "                aug_image = augmented['image']\n",
    "                \n",
    "            if torch.is_tensor(augmented['mask']):\n",
    "                aug_mask = augmented['mask'].numpy()\n",
    "                if len(aug_mask.shape) == 3:\n",
    "                    aug_mask = np.transpose(aug_mask, (1, 2, 0))\n",
    "            else:\n",
    "                aug_mask = augmented['mask']\n",
    "                \n",
    "            axes[i, 0].set_title(f\"Augmented Image {i}\")\n",
    "            axes[i, 1].set_title(f\"Augmented Mask {i}\")\n",
    "        \n",
    "        # Display the augmented images\n",
    "        axes[i, 0].imshow(aug_image.squeeze(), cmap='gray') \n",
    "        axes[i, 0].axis('off')\n",
    "        axes[i, 1].imshow(aug_mask.squeeze(), cmap='gray')\n",
    "        axes[i, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Model Architectures\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, \n",
    "                             stride=stride, padding=padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.relu(self.bn(self.conv(x)), inplace=True)\n",
    "\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size,\n",
    "                                  stride=stride, padding=padding, groups=in_channels, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.depthwise(x)), inplace=True)\n",
    "        return F.relu(self.bn2(self.pointwise(x)), inplace=True)\n",
    "\n",
    "class EfficientUNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = ConvBlock(in_channels, 32)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = DepthwiseSeparableConv(32, 64)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.enc3 = DepthwiseSeparableConv(64, 128)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.enc4 = DepthwiseSeparableConv(128, 256)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = DepthwiseSeparableConv(256, 512)\n",
    "        \n",
    "        # Decoder\n",
    "        self.upconv4 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec4 = DepthwiseSeparableConv(512, 256)\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec3 = DepthwiseSeparableConv(256, 128)\n",
    "        self.upconv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec2 = DepthwiseSeparableConv(128, 64)\n",
    "        self.upconv1 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.dec1 = DepthwiseSeparableConv(64, 32)\n",
    "        \n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(32, out_channels, kernel_size=1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(self.pool1(enc1))\n",
    "        enc3 = self.enc3(self.pool2(enc2))\n",
    "        enc4 = self.enc4(self.pool3(enc3))\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.dec4(dec4)\n",
    "        \n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.dec3(dec3)\n",
    "        \n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.dec2(dec2)\n",
    "        \n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.dec1(dec1)\n",
    "        \n",
    "        # Output segmentation map\n",
    "        return torch.sigmoid(self.outconv(dec1))\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Loss Functions\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, weight=0.5):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "        self.weight = weight\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        # Ensure targets are float type\n",
    "        targets = targets.float()\n",
    "        \n",
    "        # Ensure same shape between inputs and targets\n",
    "        if len(targets.shape) == 3:\n",
    "            targets = targets.unsqueeze(1)  # Add channel dimension\n",
    "        \n",
    "        # Ensure all values are in [0, 1] range\n",
    "        targets = torch.clamp(targets, 0, 1)\n",
    "            \n",
    "        # BCE Loss\n",
    "        bce = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        \n",
    "        # Dice Loss\n",
    "        smooth = 1e-5\n",
    "        inputs_flat = inputs.view(-1)\n",
    "        targets_flat = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs_flat * targets_flat).sum()\n",
    "        dice = (2. * intersection + smooth) / (inputs_flat.sum() + targets_flat.sum() + smooth)\n",
    "        dice_loss = 1 - dice\n",
    "        \n",
    "        # Combine losses\n",
    "        return bce * self.weight + dice_loss * (1 - self.weight)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Training Function\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, \n",
    "                num_epochs=50, device='cuda'):\n",
    "    \"\"\"Train the segmentation model\"\"\"\n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    history = {\"train_loss\": [], \"val_loss\": []}\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    # Main epoch loop with tqdm\n",
    "    epoch_bar = tqdm(range(num_epochs), desc=\"Epochs\", leave=True)\n",
    "    \n",
    "    for epoch in epoch_bar:\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # Add miniters parameter to update more frequently\n",
    "        train_bar = tqdm(train_loader, desc=f\"Train {epoch+1}/{num_epochs}\", \n",
    "                      leave=False, miniters=1)\n",
    "        \n",
    "        for i, batch in enumerate(train_bar):\n",
    "            images = batch['image'].to(device)\n",
    "            masks = batch['mask'].float().to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update running loss and progress bar description\n",
    "            batch_loss = loss.item()\n",
    "            train_loss += batch_loss\n",
    "            \n",
    "            # Update on every batch\n",
    "            train_bar.set_postfix({\"loss\": f\"{batch_loss:.4f}\"})\n",
    "            \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        # Wrap validation loader with tqdm\n",
    "        val_bar = tqdm(val_loader, desc=f\"Valid {epoch+1}/{num_epochs}\", \n",
    "                      leave=False, miniters=1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_bar:\n",
    "                images = batch['image'].to(device)\n",
    "                masks = batch['mask'].float().to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                batch_loss = criterion(outputs, masks).item()\n",
    "                \n",
    "                val_loss += batch_loss\n",
    "                val_bar.set_postfix({\"loss\": f\"{batch_loss:.4f}\"})\n",
    "                \n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        last_lr = optimizer.param_groups[0]['lr']\n",
    "        if epoch > 0 and last_lr != prev_lr:\n",
    "            print(f'Learning rate adjusted to {last_lr}')\n",
    "        prev_lr = last_lr\n",
    "        \n",
    "        # Save history\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        \n",
    "        # Update the main progress bar with epoch results\n",
    "        epoch_bar.set_postfix({\n",
    "            \"train_loss\": f\"{train_loss:.4f}\", \n",
    "            \"val_loss\": f\"{val_loss:.4f}\"\n",
    "        })\n",
    "        \n",
    "        # Log epoch results\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_nodule_detector.pth')\n",
    "            print(\"Saved best model checkpoint.\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Main Execution\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"Starting lung nodule segmentation pipeline...\")\n",
    "    \n",
    "    # Download dataset\n",
    "    print(\"Downloading dataset...\")\n",
    "    path = kagglehub.dataset_download(\"zhangweiled/lidcidri\")\n",
    "    print(\"Dataset downloaded to:\", path)\n",
    "    \n",
    "    # Set hyperparameters\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_EPOCHS = 50\n",
    "    LEARNING_RATE = 3e-4\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    \n",
    "    # Create dataset instances with appropriate augmentations\n",
    "    print(\"Creating datasets...\")\n",
    "    train_dataset = LIDCDatasetWithAugmentation(path, split='train')\n",
    "    val_dataset = LIDCDatasetWithAugmentation(path, split='val')\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Print dataset sizes\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = EfficientUNet(in_channels=1, out_channels=1)\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "    \n",
    "    # Define loss, optimizer, and scheduler\n",
    "    criterion = DiceBCELoss(weight=0.5)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model, history = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
    "        num_epochs=NUM_EPOCHS, device=DEVICE\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 1, 1)\n",
    "    plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"training_history.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    print(\"Best model saved to: best_nodule_detector.pth\")\n",
    "    print(\"Training history plot saved to: training_history.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
